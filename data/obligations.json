[
  {
    "id": "o1",
    "article": "[Article 4](https://artificialintelligenceact.eu/article/4/)",
    "description": "### Obligation for AI Literacy\n\nProviders and deployers of AI systems are required to take appropriate measures to develop and maintain AI literacy among their staff and other individuals who operate or use AI systems on their behalf. This ensures that personnel understand AI capabilities, limitations, risks, and compliance requirements, fostering responsible and knowledgeable use of AI technologies.\n\n**Compliance Task List:**\n- [ ] Develop and implement AI literacy training programs tailored to the roles of staff and operators involved with AI systems.\n- [ ] Ensure ongoing education and updates to training materials in line with evolving AI technologies and regulatory changes.\n- [ ] Maintain records of AI literacy initiatives, attendance, and competency assessments.\n- [ ] Foster a culture that promotes awareness of AI risks, ethical considerations, and compliance obligations.\n- [ ] Include AI literacy objectives within broader organizational training and governance frameworks.\n\nThis structured obligation supports organizations in demonstrating a proactive stance on AI knowledge and responsible use as mandated by the EU AI Act for providers and deployers.",
    "requiredTags": [
      "role:provider",
      "role:deployer"
    ],
    "order": 3
  },
  {
    "id": "o2",
    "article": "[Article 50](https://artificialintelligenceact.eu/article/50/)",
    "description": "### Obligation for Article 50 – Transparency Obligations for Providers and Users of Certain AI Systems  \n\n[Article 50](https://artificialintelligenceact.eu/article/50/) establishes detailed transparency obligations for providers and deployers of certain AI systems to ensure clarity, trust, and responsible use in interactions with natural persons. These obligations apply primarily to AI systems intended for direct interaction with humans, as well as systems generating or manipulating synthetic content (such as deepfakes and AI-generated text). The key objectives are to prevent deception, promote user awareness, and ensure compliance with data protection laws.\n\nProviders must design AI systems so that users are clearly informed when they are interacting with AI, unless it is obvious to a reasonably well-informed person based on context. Providers of AI systems that create or manipulate synthetic audio, image, video, or text content must label these outputs in a machine-readable format to identify their artificial origin. The technical solutions for labeling must be effective, interoperable, robust, and reliable, respecting state-of-the-art standards adapted to content type and implementation costs.\n\nDeployers, especially of limited-risk AI systems, have additional responsibilities. They must disclose when AI-generated or manipulated content is presented, notably deepfakes or text published on matters of public interest. This disclosure must be clear and unambiguous, provided at the first interaction or exposure. Exceptions apply for uses authorized by law for criminal investigations or for artistic, satirical, or fictional works where labeling is adapted to not impair the work's perception.\n\nDeployers using emotion recognition or biometric categorization systems must inform individuals exposed to such systems of their operation and comply with relevant data protection regulations (such as GDPR). These obligations do not apply when the AI system is legally permitted to detect, prevent, or investigate criminal offenses under appropriate safeguards.\n\nOverall, [Article 50](https://artificialintelligenceact.eu/article/50/) mandates proactive communication to users and technical measures that uphold transparency, ethical standards, and legal compliance in AI system deployment.\n\n**Compliance Task List:**\n- [ ] Ensure all AI systems intended for direct interaction with natural persons are designed to notify users clearly that they are interacting with an AI system, unless it is obvious from the context.\n- [ ] Implement technical solutions to label outputs of AI systems generating or manipulating synthetic content (audio, image, video, text) in machine-readable formats that reliably indicate artificial origin.\n- [ ] Maintain the effectiveness, interoperability, robustness, and reliability of labeling techniques according to current technical standards and cost-effectiveness.\n- [ ] Deployers must clearly disclose when AI-generated or manipulated content (deepfakes) is presented, especially if it concerns matters of public interest, unless exceptions apply by law or editorial responsibility.\n- [ ] Inform individuals subjected to emotion recognition or biometric categorization AI systems about the system's operation and ensure all personal data processing complies with the GDPR and other relevant laws.\n- [ ] Keep documentation of implemented transparency measures, labels, disclosures, and user notifications as evidence of compliance.\n- [ ] Update transparency practices continuously to align with evolving legal guidance, technical solutions, and industry standards.\n- [ ] For AI-generated text published to inform the public on public interest issues, disclose its artificial origin unless it has undergone human editorial review with responsibility assumed.\n- [ ] In cases of artistic, satirical, or fictional synthetic content, provide appropriate disclosure without impairing the work's enjoyment or artistic expression.\n- [ ] Monitor legal exemptions or limitations, especially for AI uses authorized for criminal justice purposes, ensuring appropriate safeguards and documentation.\n\nThis comprehensive approach helps organizations satisfy the EU AI Act's rigorous transparency mandates, fostering informed user interactions and responsible AI deployment.",
    "requiredTags": [
      "role:provider",
      "role:deployer"
    ],
    "order": 5
  },
  {
    "id": "o3",
    "article": "[Article 49](https://artificialintelligenceact.eu/article/49/), [Article 71](https://artificialintelligenceact.eu/article/71/)",
    "description": "### Obligation for Registration  \n\nProviders and authorized representatives are required to register themselves and their AI systems in the EU database established under [Article 71](https://artificialintelligenceact.eu/article/71/). This registration ensures traceability, transparency, and regulatory oversight of AI systems placed on the EU market or put into service. Accurate and timely registration helps authorities monitor compliance and facilitates effective post-market surveillance.\n\n**Compliance Task List:**\n- [ ] Register the organization as a provider or authorized representative in the designated EU AI database.\n- [ ] Ensure each AI system placed on the EU market or put into service under your name or trademark is registered.\n- [ ] Keep registration information accurate, complete, and up-to-date with any changes to the system or organization.\n- [ ] Coordinate with authorized representatives (if applicable) to ensure they fulfill their registration obligations.\n- [ ] Maintain documentation of registration submissions as evidence of compliance.\n- [ ] Monitor updates or changes to registration requirements and update entries accordingly.\n\nThis obligation supports organizations in meeting essential transparency and accountability requirements under the EU AI Act.",
    "requiredTags": [
      "role:provider",
      "role:deployer",
      "role:authorized-representative"
    ],
    "requiredAllTags": [
      "risk:high"
    ],
    "order": 10
  },
  {
    "id": "o4",
    "article": "[Article 6](https://artificialintelligenceact.eu/article/6/), [Article 9](https://artificialintelligenceact.eu/article/9/)",
    "description": "### Obligation for Risk Classification and Documentation under the EU AI Act  \n\nOrganizations must classify their AI systems according to the risk level categories defined in the EU AI Act. This risk classification is essential as it determines the applicable compliance requirements and obligations. For AI systems classified as high-risk, providers are mandated to establish, implement, document, and maintain a comprehensive risk management system throughout the lifecycle of the AI system. This includes the identification, evaluation, and mitigation of known and foreseeable risks to health, safety, and fundamental rights. The classification process and associated risk management activities must be recorded and maintained to demonstrate compliance. This ensures transparency, accountability, and readiness for regulatory oversight, supporting safe and responsible AI deployment in the EU.\n\n**Compliance Task List:**\n- [ ] Conduct a thorough risk classification of each AI system based on the criteria set out in [Article 6](https://artificialintelligenceact.eu/article/6/), considering system type, intended purpose, and potential risks.\n- [ ] Document the rationale and outcome of the risk classification process for each AI system, including evidence and assessment details.\n- [ ] For high-risk AI systems, establish and maintain a risk management system as specified under [Article 9](https://artificialintelligenceact.eu/article/9/), including regular reviews and updates throughout the system lifecycle.\n- [ ] Identify and analyze known and reasonably foreseeable risks to health, safety, and fundamental rights associated with the AI system.\n- [ ] Implement and document appropriate risk mitigation measures designed to reduce residual risks to an acceptable level.\n- [ ] Include risk evaluation based on post-market monitoring data and adapt the risk management measures accordingly.\n- [ ] Maintain comprehensive records of the risk management process, risk assessments, mitigation actions, and any changes in risk classification.\n- [ ] Ensure the risk classification and risk management documentation are readily available for audit and inspection by competent authorities.\n- [ ] Integrate risk classification and risk management documentation within the overall AI governance and compliance framework.\n\nThis obligation is a foundational step in the AI Act compliance journey, safeguarding that AI systems are appropriately assessed and managed according to their risk profile as required by EU law. It supports the organization’s accountability and helps prevent harm linked to AI system deployment.\n",
    "requiredTags": [
      "role:provider",
      "role:deployer",
      "role:product-manufacturer",
      "role:authorized-representative",
      "role:importer",
      "role:distributor"
    ],
    "order": 1
  },
  {
    "id": "o5",
    "article": "[Article 5](https://artificialintelligenceact.eu/article/5/), [Article 20](https://artificialintelligenceact.eu/article/20/)",
    "description": "### Obligation for Prohibited AI Systems under the EU AI Act\n\nThe EU AI Act prohibits certain AI systems and practices deemed to pose unacceptable risks that fundamentally conflict with EU values and the protection of individual rights. These prohibited AI systems include uses that manipulate or deceive individuals through subliminal techniques, exploit vulnerabilities based on age, physical or mental disability, engage in social scoring resulting in discriminatory treatment, or use biometric identification and categorization in unauthorized manners. Providers, deployers, importers, distributors, and authorized representatives must ensure such AI systems are never placed on the market or put into service within the EU, except under strictly regulated exceptions (e.g., for research or regulatory sandboxing).\n\nMoreover, when an AI system is identified as prohibited or falls outside compliance, the company must immediately cease its use and deployment, duly notify competent authorities including the European AI Office, and cooperate with investigations. The company is also encouraged to seek guidance from the AI Office or relevant regulatory bodies on how to adapt or redesign the system to bring it back into compliance with the AI Act.\n\nThis obligation is essential to uphold fundamental rights, protect public safety, and prevent harm from unacceptable AI technologies.\n\n**Compliance Task List:**\n- [ ] Rigorously assess AI systems to identify any that fall under the prohibited practices defined in Article 5.\n- [ ] Immediately stop development, deployment, distribution, or use of any AI system classified as prohibited.\n- [ ] Notify the relevant national market surveillance authorities as well as the European AI Office about the detection of prohibited AI systems.\n- [ ] Document all assessments, decisions, and communications relevant to prohibited AI systems.\n- [ ] Cooperate fully with investigations and enforcement actions by both national authorities and the European AI Office.\n- [ ] Seek guidance, technical advice, and support from the European AI Office or authorized regulatory bodies for possible corrective actions or system redesign.\n- [ ] Implement robust internal governance and controls to prevent the inadvertent creation or use of prohibited AI systems.\n- [ ] Maintain records of all compliance activities and corrective measures as evidence for audits and inspections.\n- [ ] Stay informed on regulatory developments and guidance updates related to prohibited AI practices.\n\nBy adhering to this comprehensive set of obligations, organizations demonstrate full accountability and responsibility, ensuring their AI systems respect EU regulatory standards and ethical principles.",
    "requiredTags": [
      "risk:prohibited"
    ],
    "order": 1
  },
  {
    "id": "o6",
    "article": "[Article 6(3)](https://artificialintelligenceact.eu/article/6/), [Article 6(4)](https://artificialintelligenceact.eu/article/6/)",
    "description": "### Obligation for Applying the Derogation Clause under Article 6(3) and (4) of the EU AI Act\n\nProviders and deployers of AI systems listed in Annex III may apply a derogation from high-risk classification under Article 6(3) if their AI system does not pose a significant risk of harm to health, safety, or fundamental rights and meets specific conditions. These conditions include performing narrow procedural tasks, improving prior human decisions without independently deciding, detecting decision patterns without replacing human review, or conducting preparatory tasks related to Annex III use cases.\n\nArticle 6(4) requires organizations that apply this derogation to maintain clear and detailed documentation of the assessment supporting the claim that the AI system is not high-risk. This documentation must be available for competent authorities to verify the compliance status and must justify why the system is exempted from high-risk obligations.\n\n**Compliance Task List:**\n- [ ] Conduct a detailed assessment to determine if the AI system listed in Annex III meets any of the derogation conditions specified in Article 6(3):\n  - Performing only narrow procedural tasks within broader processes.\n  - Improving results of a previously completed human activity without autonomous decision-making.\n  - Detecting decision-making patterns without replacing or materially influencing prior human assessment, which must include human review.\n  - Conducting preparatory tasks relevant to Annex III use cases.\n- [ ] Confirm that the AI system does not pose significant risks to health, safety, or fundamental rights, and that it does not materially influence decision-making outcomes.\n- [ ] Document the entire assessment process, including rationale, evidence, and outcomes justifying the derogation claim.\n- [ ] Retain this documentation and keep it readily accessible for inspection by regulatory authorities.\n- [ ] Update the assessment documentation promptly if any changes occur that might affect the risk classification.\n- [ ] Communicate internally the derogation status and ensure corresponding compliance measures reflect the exempted risk level.\n- [ ] Be prepared to provide evidence of this derogation claim in case of audits, market surveillance checks, or inquiries by competent authorities.\n\nThis obligation ensures that organizations responsibly and transparently apply the derogation clause in Article 6, demonstrating sound risk assessment practices and maintaining clear evidence justifying their AI system’s risk classification.",
    "requiredTags": [
      "role:provider",
      "role:deployer"
    ],
    "requiredAllTags": [
      "risk:high"
    ]
  },
  {
    "id": "o7",
    "article": "[Article 8](https://artificialintelligenceact.eu/article/8/)",
    "description": "### Obligation for Article 8 – Compliance with the Requirements for High-Risk AI Systems\n\nProviders of high-risk AI systems must demonstrate full compliance with all applicable requirements outlined in the EU AI Act. This includes establishing, implementing, and maintaining appropriate technical and organizational measures to ensure their AI systems meet legal standards related to risk management, data governance, transparency, human oversight, accuracy, cybersecurity, and documentation. Article 8 sets the framework for conformity assessment procedures that providers must follow before placing their high-risk AI systems on the EU market or putting them into service.\n\nCompliance demonstration involves rigorous and ongoing activities such as preparing technical documentation, conducting conformity assessments (internal or involving notified bodies), maintaining quality management systems, and ensuring traceability through registration and record-keeping. The objective is to provide clear, auditable evidence that the AI system adheres to the high standards mandated by the regulation, thus safeguarding safety, fundamental rights, and user trust.\n\n**Compliance Task List:**\n- [ ] Develop and implement technical and organizational measures to fulfil the AI Act’s mandatory requirements for high-risk AI systems.\n- [ ] Conduct conformity assessments as specified by the AI Act to verify compliance, engaging notified bodies when required.\n- [ ] Prepare and maintain comprehensive technical documentation covering design, development, training data, risk management, validation, human oversight, and cybersecurity.\n- [ ] Establish and operate a quality management system to ensure consistent fulfillment of compliance obligations.\n- [ ] Register the high-risk AI system in the EU database before market placement or service.\n- [ ] Affix the CE marking and draw up a Declaration of Conformity, retaining records for at least 10 years.\n- [ ] Ensure traceability through proper record-keeping of logs, incident reports, and monitoring data.\n- [ ] Review and update compliance measures regularly, including after modifications or new versions of the AI system.\n- [ ] Cooperate fully with national authorities and the European AI Office during audits, assessments, or investigations.\n- [ ] Train relevant personnel on compliance obligations and maintain documentation of such training.\n- [ ] Monitor developments in harmonized standards, common specifications, and regulatory guidance, adapting compliance frameworks accordingly.\n\nBy fulfilling these obligations, providers demonstrate their commitment to deploying high-risk AI systems that meet EU legal, ethical, and safety standards, thereby enabling market access and building trust among users and regulators.",
    "requiredAllTags": [
      "risk:high",
      "role:provider"
    ]
  },
  {
    "id": "o8",
    "article": "[Article 9](https://artificialintelligenceact.eu/article/9/)",
    "description": "### Obligation for Article 9 – Risk Management Systems\n\nProviders of high-risk AI systems are required to establish, implement, document, and maintain a comprehensive risk management system throughout the entire lifecycle of the AI system. This system must be proactive and continuous, identifying, analyzing, evaluating, and mitigating known and foreseeable risks related to health, safety, and fundamental rights. The risk management process should be integrated into the design, development, testing, and deployment phases, ensuring that residual risks are reduced to an acceptable level. Providers must regularly review and update the risk management system based on new information, operational experience, or changes in the system or its environment.\n\nImplementing a risk management system per Article 9 is critical to safeguard users and affected persons, ensure regulatory compliance, and foster trust in high-risk AI technologies.\n\n**Compliance Task List:**\n- [ ] Develop a structured risk management plan tailored to the AI system’s intended purpose and context of use.\n- [ ] Identify and analyze potential risks to health, safety, and fundamental rights throughout the AI system’s lifecycle.\n- [ ] Evaluate the severity and likelihood of identified risks to determine their significance.\n- [ ] Implement risk mitigation measures designed to reduce unacceptable risks to an acceptable residual level.\n- [ ] Document all risk management activities, including risk assessments, decisions, implemented controls, and rationale.\n- [ ] Continuously monitor the AI system’s performance, new risks, and emerging issues through post-market monitoring and user feedback.\n- [ ] Update the risk management system accordingly to address new findings or system changes.\n- [ ] Ensure risk management documentation is comprehensive, auditable, and readily available for competent authorities.\n- [ ] Integrate risk management with other compliance processes such as technical documentation, human oversight, and cybersecurity.\n- [ ] Train relevant staff on risk management policies and procedures to embed a risk-aware culture.\n\nBy fulfilling these obligations, providers demonstrate that they have a robust process to anticipate, control, and minimize risks associated with high-risk AI systems, fulfilling the safety and ethical standards mandated by the EU AI Act.",
    "requiredAllTags": [
      "risk:high",
      "role:provider"
    ]
  },
  {
    "id": "o9",
    "article": "[Article 10](https://artificialintelligence.eu/about-the-act/article-10)",
    "description": "### Obligation for Article 10 – Data and Data Governance\n\nFor **high-risk AI systems** employing training, validation, and testing datasets, providers must implement **robust data governance and management practices** to ensure the datasets meet stringent quality criteria including correctness, relevance, completeness, and representativeness. This encompasses documenting key design decisions; detailing data collection methods and origin; applying thorough preparation procedures such as annotation, labelling, cleaning, updating, and augmentation; and assessing and mitigating any bias or gaps in the datasets. Providers must also ensure special safeguards for handling sensitive personal data, including compliance with privacy and security regulations. Furthermore, datasets should be tailored to the intended use environment, taking into account geographical, contextual, and behavioural specifics to minimize risks related to safety, fairness, and accuracy. These processes should be maintained continuously to adapt to evolving data and operational conditions.\n\n**Compliance Task List:**\n\n- [ ] Develop and implement comprehensive data governance policies covering all stages of data lifecycle: collection, preparation, storage, updating, and management for training, validation, and testing datasets.\n- [ ] Document all relevant information about datasets, including sources, data types, data origin, and annotation and labelling methodologies.\n- [ ] Ensure datasets are relevant to the AI system’s intended purpose and are statistically representative, complete, and free from errors.\n- [ ] Conduct assessments to identify potential biases and gaps in the data that may lead to unfair or harmful outcomes and apply mitigation techniques documented accordingly.\n- [ ] Apply specific safeguards and protections when processing sensitive or special categories of personal data, in line with data protection laws and standards.\n- [ ] Maintain ongoing monitoring and update procedures for datasets to ensure continuous compliance and data quality throughout the lifecycle of the AI system.\n- [ ] Provide complete and precise documentation of data governance and datasets as part of the technical documentation for conformity assessment and regulatory inspections.\n- [ ] Coordinate governance and quality assurance with third-party data providers to ensure supply chain integrity and compliance.\n- [ ] Train relevant personnel in data governance practices and maintain evidence of such training.\n- [ ] Implement cybersecurity controls to protect data integrity, confidentiality and ensure secure data handling throughout AI system development and deployment.\n\nThis obligation helps organizations proactively manage risks associated with data quality and governance, which are critical for the trustworthy and compliant operation of high-risk AI systems under the EU Artificial Intelligence Act.",
    "requiredAllTags": [
      "risk:high",
      "role:provider"
    ]
  },
  {
    "id": "o10",
    "article": "[Article 11](https://artificialintelligenceact.eu/article/11/)",
    "description": "### Obligation for Article 11 – Technical Documentation\n\nProviders of high-risk AI systems must prepare and maintain comprehensive technical documentation before placing their system on the EU market or putting it into service. This documentation serves to demonstrate conformity with the EU AI Act’s requirements and must cover all relevant aspects of the AI system’s design, development, testing, and deployment. It includes detailed descriptions of the system's intended purpose and functionalities, design and development processes, data governance, risk management, human oversight, performance evaluation, cybersecurity measures, and compliance with applicable harmonized standards and common specifications.\n\nThe technical documentation ensures full transparency and traceability of the AI system’s compliance posture and must be kept up to date throughout the system's lifecycle to facilitate audits, conformity assessments, and regulatory inspections by competent authorities.\n\n**Compliance Task List:**\n- [ ] Prepare detailed technical documentation encompassing the AI system's intended purpose, scope, and functional characteristics.\n- [ ] Document design choices, development methodologies, algorithms, and data used (training, validation, testing datasets) with provenance and quality controls.\n- [ ] Include comprehensive risk management records demonstrating identification, evaluation, and mitigation of risks to health, safety, and fundamental rights.\n- [ ] Describe implemented human oversight measures and explain how these enable effective human control over the system.\n- [ ] Provide evidence of performance testing, including accuracy, robustness, and cybersecurity evaluations.\n- [ ] Document compliance with relevant harmonized standards, common specifications, and other regulatory requirements.\n- [ ] Keep the technical documentation current by updating it after modifications, new versions, or relevant changes in applicable regulations or standards.\n- [ ] Ensure the technical documentation is accessible and available in a format suitable for review by notified bodies and national competent authorities upon request.\n- [ ] Maintain the technical documentation securely for at least 10 years after the AI system is placed on the market or put into service.\n- [ ] Integrate the technical documentation process into the overall quality management and compliance systems of the organization.",
    "requiredAllTags": [
      "risk:high",
      "role:provider"
    ]
  },
  {
    "id": "o11",
    "article": "[Article 12](https://artificialintelligenceact.eu/article/12/)",
    "description": "### Obligation for Article 12 – Record-Keeping\n\nProviders of high-risk AI systems must implement automated record-keeping mechanisms that continuously and securely log relevant events throughout the entire lifecycle of the AI system. These logs are critical for ensuring traceability, enabling effective post-market monitoring, incident investigation, and compliance verification by authorities. The record-keeping system should capture information such as system inputs, outputs, interactions, decisions made, and any changes or updates to the system. Logs must be protected against unauthorized access, tampering, and loss, and retained for a minimum period as required by the regulation. Proper record-keeping supports transparency, accountability, and responsiveness in managing AI risks.\n\n**Compliance Task List:**\n- [ ] Establish automated logging processes that capture detailed and relevant event data from the AI system’s operation lifecycle.\n- [ ] Ensure logs record inputs, outputs, user interactions, decision-making processes, and system modifications.\n- [ ] Protect logs against unauthorized access, tampering, deletion, and data loss through appropriate security controls.\n- [ ] Retain all logs for at least six months or the period mandated by competent authorities.\n- [ ] Make logs readily accessible for internal review, audits, investigations, and regulatory inspections.\n- [ ] Regularly review and update record-keeping systems to address evolving technical and regulatory requirements.\n- [ ] Document the record-keeping procedures, technologies used, and responsible personnel within the organization.\n- [ ] Train relevant staff on the importance and handling of record-keeping obligations.\n- [ ] Coordinate record-keeping practices with post-market monitoring and incident reporting duties to ensure seamless compliance.\n\nThis obligation ensures that providers of high-risk AI systems maintain a transparent and auditable operational history, facilitating accountability and supporting regulatory oversight as required by the EU AI Act.",
    "requiredAllTags": [
      "risk:high",
      "role:provider"
    ]
  },
  {
    "id": "o13",
    "article": "[Article 13](https://artificialintelligenceact.eu/article/13/)",
    "description": "### Obligation for Article 13 – Transparency and Provision of Information to Deployers  \n\nArticle 13 mandates that providers of high-risk AI systems must supply deployers with clear, comprehensive, and accessible information—commonly known as \"instructions for use\"—to ensure safe and effective deployment. These instructions must enable deployers to understand the system's capabilities, limitations, intended purpose, and the essential requirements to maintain compliance throughout the use phase.\n\nThe information provided should cover technical specifications, installation and maintenance guidelines, human oversight measures, risk management details, and specific data governance requirements. Providers must ensure that this information is transparent, understandable, and easily accessible to deployers to facilitate proper use, oversight, and risk mitigation of high-risk AI systems in real-world environments.\n\n**Compliance Task List:**\n- [ ] Prepare detailed and up-to-date \"instructions for use\" for the high-risk AI system, tailored for deployers.\n- [ ] Include explanations of the AI system’s intended purpose, operational principles, and functional limits.\n- [ ] Provide technical specifications necessary for correct installation, maintenance, and operation.\n- [ ] Describe required human oversight measures to ensure appropriate control and intervention.\n- [ ] Outline relevant risk management procedures and any residual risks deployers should be aware of.\n- [ ] Detail data governance and data protection requirements related to the system’s use.\n- [ ] Ensure the instructions are written in clear, understandable language suitable for intended deployer audiences.\n- [ ] Make information readily accessible in digital or physical format as appropriate.\n- [ ] Update instructions promptly to reflect any changes in the AI system, risk assessments, or regulatory requirements.\n- [ ] Maintain records of the information provided to deployers as evidence of compliance.\n- [ ] Support deployers with supplementary materials or training if necessary to ensure proper use.\n\nThis obligation is critical to empower deployers with knowledge and tools to safely operate high-risk AI systems while maintaining regulatory compliance and protecting fundamental rights and safety.",
    "requiredTags": [
      "role:provider",
      "role:deployer",
      "role:authorized-representative"
    ],
    "requiredAllTags": [
      "risk:high"
    ]
  },
  {
    "id": "o17",
    "article": "[Article 14](https://artificialintelligenceact.eu/article/14/)",
    "description": "### Obligation for Article 14 – Human Oversight   \n\nProviders of high-risk AI systems must implement effective human oversight measures that are proportionate to the level of risk, the degree of autonomy of the AI system, and the specific context in which the system is used. Human oversight ensures that AI systems operate safely, reliably, and in alignment with fundamental rights by enabling human intervention, monitoring, and control at appropriate points in the AI system's lifecycle.\n\nThese oversight measures must be integrated into the design and deployment of the AI system to prevent or mitigate risks that may arise from system failures, errors, or unintended consequences. Providers must clearly define the roles, responsibilities, and capabilities required of human overseers, who should have the necessary information and means to understand, interpret, and intervene in the operation of the AI system effectively. The aim is to maintain human dignity, protect fundamental rights, and ensure accountability by avoiding over-reliance on AI decisions without meaningful human judgment.\n\n**Compliance Task List:**\n- [ ] Design and implement human oversight mechanisms tailored to the AI system’s risk profile, autonomy level, and use context.\n- [ ] Define clear roles and responsibilities for human overseers, ensuring their ability to effectively monitor and intervene.\n- [ ] Provide human overseers with sufficient information, tools, and training to understand AI system outputs and potential risks.\n- [ ] Ensure that human oversight can prevent, mitigate, or correct potentially harmful outcomes from AI operation.\n- [ ] Integrate human-in-the-loop, human-on-the-loop, or human-in-command approaches as appropriate for the system’s function.\n- [ ] Document the human oversight framework and its implementation, including decisions about the choice and extent of oversight measures.\n- [ ] Continuously evaluate and update human oversight measures in response to system modifications, operational experience, or emerging risks.\n- [ ] Facilitate transparency and explainability features to support human understanding and decision-making.\n- [ ] Ensure compliance with related requirements such as risk management, transparency, and data governance that underpin effective human oversight.\n- [ ] Retain records of oversight practices and training to demonstrate ongoing compliance and accountability for audits and inspections.\n\nThis obligation ensures that high-risk AI systems operate under robust human control, preserving safety, fundamental rights, and trustworthiness as mandated by the EU AI Act.",
    "requiredTags": [
      "role:provider",
      "role:deployer"
    ],
    "requiredAllTags": [
      "risk:high"
    ]
  },
  {
    "id": "o15",
    "article": "[Article 15](https://artificialintelligenceact.eu/article/15/)",
    "description": "### Obligation for Article 15 – Accuracy, Robustness, and Cybersecurity\n\nProviders of high-risk AI systems must ensure that their systems are designed and developed with technical measures that guarantee accuracy, robustness, and cybersecurity throughout the entire lifecycle of the AI system. Accuracy pertains to the system’s ability to produce reliable and precise outputs aligned with its intended purpose. Robustness requires the AI system to remain resilient to errors, failures, environmental variations, and attempts to manipulate or attack it. Cybersecurity measures must protect the system against unauthorized access, data breaches, adversarial attacks, and other cyber threats.\n\nThese technical safeguards are essential to prevent malfunctions, unintended behaviors, and security vulnerabilities that could compromise health, safety, or fundamental rights. Providers need to integrate continuous testing, validation, and monitoring of the AI system’s performance under diverse conditions to detect and address weaknesses proactively. Ensuring accuracy, robustness, and cybersecurity supports trustworthiness, regulatory compliance, and user safety.\n\n**Compliance Task List:**\n- [ ] Implement design and development practices that ensure the AI system achieves and maintains high levels of accuracy relevant to its intended use.\n- [ ] Conduct rigorous testing and validation to verify the system’s performance, including robustness under expected and unexpected operating conditions.\n- [ ] Develop resilience against faults, errors, and attempts to manipulate the AI system’s functioning, including adversarial attacks and hacking.\n- [ ] Establish cybersecurity controls covering data protection, unauthorized access prevention, and incident response mechanisms.\n- [ ] Continuously monitor the AI system during operation to detect inaccuracies, performance degradation, security threats, or anomalies.\n- [ ] Update and improve the system based on observed deficiencies or new vulnerabilities identified through post-market monitoring.\n- [ ] Document technical choices, testing results, and cybersecurity measures as part of the technical documentation required by the AI Act.\n- [ ] Train relevant personnel on cybersecurity best practices and integrate security considerations into the AI development lifecycle.\n- [ ] Coordinate with external experts and use up-to-date standards and tools for robustness, accuracy, and cybersecurity assurance.\n- [ ] Ensure traceability and accountability of all efforts related to accuracy, robustness, and cybersecurity throughout the AI system’s lifecycle.\n\nBy fulfilling this obligation, providers demonstrate that the high-risk AI system is built to be reliable, secure, and resilient, thereby minimizing risks to users and the public and complying with the stringent technical standards mandated by the EU AI Act.",
    "requiredTags": [
      "role:provider",
      "risk:high"
    ],
    "requiredAllTags": [
      "risk:high"
    ]
  },
  {
    "id": "o18",
    "article": "[Article 16](https://artificialintelligenceact.eu/article/16/)",
    "description": "### Obligation for Article 16 – Obligations of Providers of High-Risk AI Systems  \n\nArticle 16 sets out the comprehensive obligations that providers of high-risk AI systems must fulfill to ensure full regulatory compliance under the EU AI Act. These obligations encompass the preparation and maintenance of technical documentation, operation of an effective quality management system, conducting conformity assessments, registration of AI systems, and various administrative duties essential for market access and ongoing compliance.\n\nProviders must establish and operate a quality management system that governs design, development, production, and post-market monitoring to maintain consistent compliance. They are responsible for ensuring thorough technical documentation is prepared and kept up to date, evidencing adherence to all relevant requirements. Before placing high-risk AI systems on the market or putting them into service, providers must complete the appropriate conformity assessment procedures, which may involve third-party notified bodies depending on the system’s characteristics.\n\nRegistration of the AI system in the designated EU database is mandatory, ensuring traceability and accountability. Providers must also cooperate with national authorities and the European AI Office by providing requested information and allowing inspections or checks. They need to monitor the performance and safety of AI systems post-market and take corrective actions when necessary.\n\nBy fulfilling these obligations, providers demonstrate their ongoing accountability and commitment to developing and maintaining AI systems that meet the high standards of safety, transparency, quality, and fundamental rights protection set out in the AI Act.\n\n**Compliance Task List:**\n- [ ] Establish and maintain a comprehensive quality management system covering the lifecycle of the high-risk AI system, including design, development, production, and post-market monitoring.\n- [ ] Prepare, update, and retain full technical documentation as required by the AI Act, ensuring it reflects the current status and design of the AI system.\n- [ ] Conduct the applicable conformity assessment procedures prior to placing the AI system on the EU market or putting it into service, engaging notified bodies if required.\n- [ ] Register the high-risk AI system and provider details in the EU AI database before market placement or service commencement.\n- [ ] Implement processes for ongoing post-market monitoring to track the AI system’s performance, safety, and compliance.\n- [ ] Promptly address any issues, defects, or non-compliance identified during post-market monitoring by taking corrective or preventive actions.\n- [ ] Cooperate fully with national competent authorities and the European AI Office by providing access to documentation, information, and facilitating inspections or audits.\n- [ ] Ensure records of all compliance activities, conformity assessments, quality management processes, and post-market monitoring are securely maintained for audit and inspection purposes.\n- [ ] Keep abreast of changes to applicable EU legislation, harmonized standards, and guidance, updating compliance measures accordingly.\n- [ ] Train and inform relevant personnel on their roles and responsibilities related to the obligations under Article 16.\n\nThis obligation is fundamental for ensuring that providers of high-risk AI systems take full responsibility for their products throughout their lifecycle, supporting a trustworthy, safe, and legally compliant AI ecosystem in the EU.",
    "requiredTags": [
      "role:provider",
      "role:product-manufacturer"
    ],
    "requiredAllTags": [
      "risk:high"
    ]
  },
  {
    "id": "o19",
    "article": "[Article 17](https://artificialintelligenceact.eu/article/17/)",
    "description": "### Obligation for Article 17 – Quality Management System  \n\nProviders of high-risk AI systems must establish, implement, and maintain a comprehensive quality management system (QMS) that ensures consistent compliance with the EU AI Act throughout the entire lifecycle of the AI system. The QMS must include documented policies, procedures, and instructions that govern all relevant processes tied to the design, development, production, deployment, and post-market monitoring of the AI system.\n\nThis system supports the organization in systematically managing quality standards, mitigating risks, ensuring traceability, and maintaining conformity with legal and technical requirements. Key elements of the QMS include clear responsibilities and roles within the organization, rigorous control of processes and documentation, continuous improvement mechanisms, and integration with other compliance activities such as risk management and technical documentation. The QMS must be adequate to the complexity and risk level of the AI system and be regularly reviewed and updated to reflect changes in technologies, regulations, or operational experience.\n\n**Compliance Task List:**\n- [ ] Develop and document a quality management system tailored to the nature and risk profile of the high-risk AI system.\n- [ ] Define policies, procedures, roles, and responsibilities to ensure consistent implementation of quality principles across all AI system lifecycle stages.\n- [ ] Integrate the QMS with other regulatory compliance processes, including risk management, technical documentation, conformity assessments, and post-market monitoring.\n- [ ] Maintain detailed records and documentation to demonstrate QMS effectiveness and adherence during audits and inspections.\n- [ ] Regularly review, assess, and update the QMS in response to new regulatory requirements, technological changes, or operational feedback.\n- [ ] Train relevant personnel on the QMS policies and procedures to embed a culture of quality and compliance.\n- [ ] Ensure that the QMS supports traceability, accountability, and continuous improvement in the AI system’s quality and safety.\n- [ ] Facilitate internal and external audits through transparent and readily available quality documentation.\n- [ ] Assign clear accountability for managing and maintaining the QMS within the organization’s governance structure.\n\nBy complying with Article 17, providers demonstrate their commitment to upholding high-quality standards, systematic risk control, and regulatory adherence, thereby ensuring the reliability and safety of high-risk AI systems in the EU market.",
    "requiredTags": [
      "role:provider"
    ],
    "requiredAllTags": [
      "risk:high"
    ]
  },
  {
    "id": "o20",
    "article": "[Article 18](https://artificialintelligenceact.eu/article/18/)",
    "description": "### Obligation for Article 18 – Documentation Keeping  \n\nProviders of high-risk AI systems must ensure that all technical documentation required under Article 11 is kept readily available and accessible for national competent authorities for a minimum period of 10 years after the AI system is placed on the EU market or put into service. This obligation guarantees that authorities can carry out effective audits, inspections, and conformity assessments during and after the system’s market presence. The documentation must be maintained in a manner that ensures its integrity, confidentiality, and accessibility throughout this period. Providers are responsible for organizing and safeguarding the documentation to promptly respond to information requests or regulatory investigations.\n\n**Compliance Task List:**\n- [ ] Securely store and maintain all technical documentation prepared pursuant to Article 11 for at least 10 years after market placement or service initiation.\n- [ ] Ensure documentation is complete, up-to-date, and organized for quick retrieval during audits, inspections, or inquiries by national competent authorities.\n- [ ] Protect documentation against loss, unauthorized access, tampering, or destruction throughout the retention period.\n- [ ] Establish internal policies and procedures for managing document retention, updates, and archiving in compliance with legal requirements.\n- [ ] Assign responsible personnel or teams for ongoing management and availability of the documentation.\n- [ ] Implement secure digital and/or physical storage solutions that comply with data protection and confidentiality requirements.\n- [ ] Regularly review and verify the integrity and accessibility of stored documentation.\n- [ ] Prepare for cooperation with national competent authorities by facilitating access to documentation as needed.\n- [ ] Maintain records of any updates or amendments made to the documentation during the retention period.\n- [ ] Include documentation retention as part of the overall quality management and compliance systems within the organization.\n\nThis obligation ensures transparent and accountable management of technical compliance evidence, supporting regulatory oversight and trust in the safety and legality of high-risk AI systems in the European market.",
    "requiredTags": [
      "role:provider",
      "role:authorized-representative"
    ],
    "requiredAllTags": [
      "risk:high"
    ]
  },
  {
    "id": "o21",
    "article": "[Article 19](https://artificialintelligenceact.eu/article/19/)",
    "description": "### Obligation for Article 19 – Automatically Generated Logs Retention\n\nProviders of high-risk AI systems must ensure that all automatically generated logs, as required under Article 12, are securely stored and retained for a minimum period of six months. These logs capture critical operational data such as system inputs, outputs, decisions, interactions, and modifications, facilitating transparency, traceability, post-market monitoring, and incident investigation. Maintaining these logs for at least six months supports regulatory authorities in auditing and investigating the AI system’s functioning and compliance over a meaningful timeframe, contributing to enhanced safety, accountability, and trustworthiness.\n\nProviders are responsible for implementing secure and robust logging systems that prevent unauthorized access, tampering, or loss of log data during the retention period. They must also ensure that logs remain accessible for internal review, audits, and requests from competent authorities to demonstrate ongoing compliance with the EU AI Act.\n\n**Compliance Task List:**\n- [ ] Retain all automatically generated logs related to the AI system’s operation for a minimum period of six months.\n- [ ] Ensure the logs capture essential information including inputs, outputs, decision-making processes, user interactions, and system changes.\n- [ ] Protect logs against unauthorized access, tampering, deletion, or data loss throughout the retention period via appropriate cybersecurity measures.\n- [ ] Maintain secure storage systems (digital or otherwise) that guarantee log integrity and accessibility.\n- [ ] Make logs readily accessible for internal review, auditing processes, and regulatory inspections by competent authorities.\n- [ ] Establish clear internal policies and procedures for log retention, protection, and management.\n- [ ] Train relevant staff on the importance of log maintenance, security procedures, and regulatory requirements.\n- [ ] Periodically review and update logging mechanisms and retention policies to comply with evolving technological and legal standards.\n- [ ] Coordinate log management with other compliance aspects such as post-market monitoring and incident reporting to ensure holistic oversight.\n- [ ] Document all log retention policies and practices as part of the organization’s compliance framework and quality management systems.\n\nBy fulfilling this obligation, providers ensure transparent and accountable operation of high-risk AI systems, enabling effective oversight and regulatory compliance in line with the EU AI Act’s safeguards.",
    "requiredTags": [
      "role:provider"
    ],
    "requiredAllTags": [
      "risk:high"
    ]
  },
  {
    "id": "o22",
    "article": "[Article 20](https://artificialintelligenceact.eu/article/20/)",
    "description": "### Obligation for Article 20 – Corrective Actions and Duty of Information\n\nProviders of high-risk AI systems must take immediate and effective corrective actions when they become aware that their system is not in conformity with EU AI Act requirements or poses risks to health, safety, or fundamental rights. These actions may include withdrawing, disabling, or recalling the AI system from the market or service. Providers have a legal duty to promptly inform national competent authorities and the European AI Office of any non-compliance, risks, or incidents, and to fully cooperate in monitoring, investigation, or enforcement procedures.\n\nThis obligation ensures rapid risk mitigation, continuous improvement, and a high level of public safety and trust in high-risk AI technologies.\n\n**Compliance Task List:**\n- [ ] Continuously monitor the performance and compliance status of high-risk AI systems in real-world use.\n- [ ] Promptly identify and assess any non-conformity, safety risk, or threat to fundamental rights associated with the AI system.\n- [ ] Develop and implement corrective actions (withdrawal, disabling, recall, or other technical or organizational measures) to address detected risks or non-compliance.\n- [ ] Inform relevant national competent authorities and the European AI Office without delay about non-conformity, the risks identified, and the corrective measures taken.\n- [ ] Provide comprehensive documentation and evidence of the issue, assessment, and corrective actions for regulatory review.\n- [ ] Cooperate fully with competent authorities and the European AI Office during monitoring, investigations, or enforcement actions.\n- [ ] Review root causes and update quality management, risk management, and compliance frameworks to prevent recurrence.\n- [ ] Retain records of corrective actions and communications as evidence for audits and future compliance checks.\n- [ ] Train staff on incident management, reporting duties, and emergency corrective action protocols.\n- [ ] Re-assess the AI system before re-deployment to confirm restored conformity and safety.",
    "requiredTags": [
      "role:provider",
      "role:deployer",
      "role:authorized-representative",
      "role:importer",
      "role:distributor"
    ],
    "requiredAllTags": [
      "risk:high"
    ]
  },
  {
    "id": "o23",
    "article": "[Article 21](https://artificialintelligenceact.eu/article/21/)",
    "description": "### Obligation for Article 21 – Cooperation with Competent Authorities  \n\nProviders of high-risk AI systems must fully cooperate with national competent authorities and the European AI Office when requested to demonstrate conformity with the EU AI Act. Upon receiving a reasoned request from a competent authority, providers are required to promptly supply all necessary information and documentation that substantiates compliance—this includes technical documentation, quality management records, risk management details, post-market monitoring data, and evidence of corrective actions taken if applicable. Cooperation supports transparency, effective supervision, and enables authorities to carry out proper market surveillance, audits, and investigations. Providers must ensure information is complete, accurate, accessible, and delivered without undue delay, helping maintain ongoing regulatory trust and oversight.\n\n**Compliance Task List:**\n- [ ] Prepare to promptly respond to reasoned requests from competent authorities for information or documentation regarding your high-risk AI system’s conformity.\n- [ ] Supply comprehensive, accurate, and up-to-date technical documentation demonstrating compliance with all relevant requirements.\n- [ ] Provide records of quality management, risk management, post-market monitoring, and any corrective actions carried out.\n- [ ] Ensure all supplied documentation is well-organized, easy to access, and ready for immediate regulatory review.\n- [ ] Cooperate fully with authorities during audits, inspections, investigations, or enforcement activities.\n- [ ] Assign responsible personnel to manage communications and coordinate the provision of documentation and information to authorities.\n- [ ] Track all interactions with competent authorities and retain related correspondence as part of compliance evidence.\n- [ ] Maintain ongoing readiness for regulatory inquiries by integrating cooperation protocols into your overall compliance and governance framework.\n- [ ] Update processes, records, and documentation regularly to reflect changes in system design, operation, or regulatory guidance, ensuring all information remains current.\n- [ ] Respect confidentiality, trade secrets, and data protection obligations while fulfilling cooperation requirements.",
    "requiredTags": [
      "role:provider"
    ],
    "requiredAllTags": [
      "risk:high"
    ]
  },
  {
    "id": "o24",
    "article": "[Article 22](https://artificialintelligenceact.eu/article/22/)",
    "description": "### Obligation for Article 22 – Authorized Representatives of Providers of High-Risk AI Systems\n\nProviders of high-risk AI systems who are established outside the European Union (third countries) must appoint an authorized representative located within the EU before placing their systems on the EU market or putting them into service. The authorized representative acts on behalf of the provider to perform specified regulatory tasks and serves as a contact point for competent authorities and the European AI Office.\n\nThe authorized representative’s responsibilities include ensuring compliance with the AI Act, maintaining and providing access to technical documentation, fulfilling registration obligations, coordinating post-market monitoring and corrective actions, and facilitating audits and inspections. This measure enhances accountability, traceability, and effective market surveillance for high-risk AI systems supplied from outside the EU.\n\n**Compliance Task List:**\n- [ ] Appoint an authorized representative established within the EU before placing a high-risk AI system on the EU market or putting it into service.\n- [ ] Formally document the appointment by written mandate specifying scope, responsibilities, and authority to act on the provider’s behalf for all required compliance tasks.\n- [ ] Ensure the authorized representative maintains up-to-date and complete technical documentation and can furnish it promptly to competent authorities or the European AI Office upon request.\n- [ ] Register the authorized representative and relevant high-risk AI systems in the EU AI database.\n- [ ] Cooperate with authorized representative in post-market monitoring, incident reporting, and implementation of corrective actions if required.\n- [ ] Enable the authorized representative to coordinate and communicate effectively with competent authorities for audits, inspections, and regulatory inquiries.\n- [ ] Update the mandate and compliance arrangements if the system or regulatory requirements change.\n- [ ] Retain records of all communications and compliance activities undertaken by the authorized representative for a minimum period as stipulated by the AI Act.\n- [ ] Train relevant personnel on roles and interactions with the authorized representative to ensure seamless compliance and oversight.",
    "requiredTags": [
      "role:provider",
      "role:authorized-representative"
    ],
    "requiredAllTags": [
      "risk:high"
    ]
  },
  {
    "id": "o25",
    "article": "[Article 23](https://artificialintelligenceact.eu/article/23/)",
    "description": "### Obligation for Article 23 – Obligations of Importers  \n\nImporters of high-risk AI systems into the EU must ensure that providers comply with key obligations under the EU AI Act before the systems enter the EU market or are put into service. This includes verifying that providers have conducted the required conformity assessments as detailed in Article 43 and have prepared comprehensive technical documentation in accordance with Article 11. Importers must also confirm the AI system bears the appropriate CE marking, is registered in the EU AI database, and fulfills other compliance measures. Importers act as gatekeepers, preventing non-compliant and potentially unsafe AI systems from entering the EU market, thus safeguarding public health, safety, and fundamental rights.\n\nImporters are required to maintain records, cooperate with competent authorities and the European AI Office, and take corrective actions if a system is found non-conforming. They must ensure that all associated compliance documentation (such as the Declaration of Conformity and technical files) is available for inspection and must initiate withdrawal or recall procedures if necessary.\n\n**Compliance Task List:**\n- [ ] Verify that the provider has performed the required conformity assessment as per Article 43 before importing the AI system into the EU.\n- [ ] Ensure technical documentation required by Article 11 is complete, up-to-date, and accessible for review by competent authorities.\n- [ ] Confirm the system bears the CE marking and is registered in the EU AI database.\n- [ ] Ascertain that all necessary compliance information accompanies the imported AI system, including the Declaration of Conformity.\n- [ ] Retain documentation evidencing compliance checks and actions taken as part of the import process.\n- [ ] Cooperate fully with national competent authorities and the European AI Office during audits, inspections, or market surveillance activities.\n- [ ] Monitor post-market performance of imported AI systems and coordinate corrective actions (withdrawal, disabling, recall) if non-conformity is detected.\n- [ ] Maintain records of suppliers, import activities, and compliance measures taken for each AI system.\n- [ ] Stay informed of updates to the EU AI Act, harmonized standards, and regulatory guidelines related to importer responsibilities.\n- [ ] Implement internal policies to verify ongoing provider compliance and ensure only conforming high-risk AI systems are imported.\n",
    "requiredTags": [
      "role:provider",
      "role:importer"
    ],
    "requiredAllTags": [
      "risk:high"
    ]
  },
  {
    "id": "o26",
    "article": "[Article 24](https://artificialintelligenceact.eu/article/24/)",
    "description": "### Obligation for Article 24 – Obligations of Distributors  \n\nDistributors of high-risk AI systems in the EU must ensure that providers and importers comply with all applicable obligations—specifically those outlined in Article 16 (Providers) and Article 23 (Importers) of the EU AI Act. Distributors are responsible for verifying that high-risk AI systems are accompanied by all required conformity documentation, have undergone proper conformity assessment, bear CE marking, and are registered in the EU AI database. Distributors must not make available on the market any high-risk AI system if they know, or ought to know, that it is non-compliant. They must also maintain traceability of products supplied, cooperate fully with national competent authorities and the European AI Office, and initiate corrective actions (such as withdrawal or recall) if compliance issues are discovered.\n\nDistributors act as crucial safeguards against non-conforming high-risk AI systems entering or circulating within the EU market, reinforcing collective responsibility for safety, compliance, and protection of fundamental rights.\n\n**Compliance Task List:**\n- [ ] Verify that providers and importers have fulfilled their obligations under Article 16 and Article 23 before making high-risk AI systems available on the EU market.\n- [ ] Ensure the high-risk AI system is accompanied by all required documentation, such as the Declaration of Conformity, CE marking, and technical files.\n- [ ] Check that the AI system is registered in the EU AI database and complies with conformity assessment procedures.\n- [ ] Do not distribute high-risk AI systems if you know, or reasonably should know, they do not meet EU AI Act requirements.\n- [ ] Maintain records of suppliers, products distributed, and compliance checks conducted.\n- [ ] Cooperate fully with national competent authorities and the European AI Office for audits, inspections, and market surveillance activities.\n- [ ] Implement corrective actions promptly—including withdrawal or recall—upon identification of non-conformity or safety risks.\n- [ ] Train relevant staff on distributor obligations, compliance checks, and incident reporting procedures.\n- [ ] Stay updated on regulatory changes and harmonized standards relevant to high-risk AI systems and distributor responsibilities.\n",
    "requiredTags": [
      "role:provider",
      "role:importer",
      "role:distributor"
    ],
    "requiredAllTags": [
      "risk:high"
    ]
  },
  {
    "id": "o27",
    "article": "[Article 26](https://artificialintelligenceact.eu/article/26/)",
    "description": "### Obligation for Article 26 – Obligations of Deployers of High-Risk AI Systems  \n\nDeployers of high-risk AI systems in the European Union bear significant responsibilities to ensure the safe, lawful, and ethical operation of these systems. Article 26 mandates that deployers implement appropriate technical and organizational measures tailored to the nature of the AI system and the risks it poses. These measures are designed to maintain compliance with the EU AI Act, protect fundamental rights, and manage risks effectively throughout the AI system’s deployment phase.\n\nA critical element of the deployer’s obligations is to assign and maintain effective human oversight mechanisms. Human oversight must be adequate to monitor the system’s functioning, intervene when necessary, and prevent or mitigate potential harm or unintended consequences. Deployers must ensure that personnel responsible for oversight have appropriate knowledge, training, and authority to exercise control and understanding of the AI system’s operation.\n\nFurthermore, deployers must maintain records of the measures taken, monitor system performance continuously, manage incidents or malfunctions promptly, and cooperate with providers and authorities to uphold compliance and safety. These combined responsibilities help create a governance environment where AI systems are operated responsibly, transparently, and with accountability.\n\n**Compliance Task List:**\n- [ ] Implement and maintain appropriate technical and organizational measures to manage risks associated with the deployment of high-risk AI systems.\n- [ ] Assign clear human oversight roles with adequate authority and competence to ensure effective monitoring and control of the AI system.\n- [ ] Ensure human overseers understand the AI system’s functionalities, limitations, and potential risks to make informed decisions.\n- [ ] Continuously monitor the AI system’s performance, detecting malfunctions, deviations, or risks that may arise during deployment.\n- [ ] Take timely corrective actions, including disabling or withdrawing the AI system in case of non-conformity or safety issues.\n- [ ] Maintain documentation of oversight measures, monitoring activities, incident reports, and corrective actions for accountability and audits.\n- [ ] Facilitate cooperation with providers and competent authorities for compliance verification, investigations, or enforcement.\n- [ ] Provide necessary training to staff involved in the deployment and oversight of the AI system to ensure competent operation.\n- [ ] Ensure compliance with any additional conditions or instructions specified by the provider in the system’s “instructions for use.”\n- [ ] Keep abreast of regulatory updates and evolving best practices to continuously improve deployment and oversight protocols.",
    "requiredTags": [
      "role:provider",
      "role:deployer",
      "role:importer",
      "role:distributor"
    ],
    "requiredAllTags": [
      "risk:high"
    ]
  },
  {
    "id": "o28",
    "article": "[Article 25](https://artificialintelligenceact.eu/article/25/)",
    "description": "### Obligation for Article 25 – Responsibilities Along the AI Value Chain  \n\nArticle 25 of the EU AI Act establishes that certain parties in the AI value chain—such as distributors, importers, deployers, or other third parties—may be considered as providers of a high-risk AI system under specific circumstances. When they take on the role of provider, they assume the full set of obligations imposed on providers under Article 16, including compliance with all relevant AI Act requirements.\n\nThis provision ensures that accountability follows the entity that controls the AI system’s deployment or substantially modifies it, thus closing potential regulatory gaps along the supply chain. It applies when any of the following conditions occur to a high-risk AI system already placed on the market or put into service:\n\n- The party puts their name, trademark, or brand on the AI system, effectively rebranding it.\n- The party makes substantial modifications to the AI system’s functionality or safety that maintain its classification as high-risk.\n- The party modifies the AI system’s intended purpose such that a system not previously classified as high-risk becomes classified as high-risk.\n\nWhen these conditions are met, the new responsible party is legally recognized as the provider, taking over all provider duties, while the original provider is relieved from these obligations except for cooperation duties, such as providing information and technical support to enable compliance.\n\nThis role assignment is critical to avoid regulatory evasion and ensures that responsibility for compliance, safety, and fundamental rights protection is clearly attributed along the AI system’s lifecycle, regardless of changes in market actors or system adaptations.\n\n**Compliance Task List:**\n- [ ] Assess whether your role as distributor, importer, deployer, or third party changes to a provider role based on any of the following:\n  - You put your name, trademark, or brand on an existing high-risk AI system.\n  - You make substantial modifications to an existing high-risk AI system’s function or safety.\n  - You modify an AI system’s intended purpose so that it becomes high-risk under the AI Act.\n- [ ] If considered a provider, ensure compliance with all relevant requirements for providers under Article 16, including documentation, conformity assessment, quality management, registration, post-market monitoring, and corrective actions.\n- [ ] Formally document any changes to the AI system or role that trigger provider status to clearly demonstrate regulatory awareness and compliance intent.\n- [ ] Ensure cooperation with the original provider by requesting and maintaining necessary technical information and support to meet provider obligations.\n- [ ] Update internal processes and compliance frameworks to reflect the transition from distributor/importer/deployer to provider status.\n- [ ] Communicate changes in provider status to relevant stakeholders, including regulatory authorities, to maintain transparency.\n- [ ] Train staff and relevant personnel on the implications of becoming a provider under Article 25 to avoid unintentional non-compliance.\n- [ ] Monitor any further changes to the AI system or its use that might affect risk classification or provider responsibilities.\n- [ ] Maintain records and evidence of conformity with provider obligations for audit and inspection purposes.\n",
    "requiredTags": [
      "role:provider",
      "role:deployer",
      "role:product-manufacturer",
      "role:importer",
      "role:distributor"
    ],
    "requiredAllTags": [
      "risk:high"
    ]
  },
  {
    "id": "o29",
    "article": "[Article 27](https://artificialintelligenceact.eu/article/27/)",
    "description": "### Obligation for Article 27 – Fundamental Rights Impact Assessments for High-Risk AI Systems\n\nDeployers of high-risk AI systems are required to conduct thorough fundamental rights impact assessments (FRIAs) to evaluate how their AI systems may affect the fundamental rights of individuals. These assessments must identify, analyze, and document specific risks of harm to rights such as privacy, non-discrimination, freedom of expression, and other protected rights under EU law. The FRIA process helps deployers understand potential adverse impacts arising from the AI system’s operation and implement measures to mitigate those risks.\n\nOnce the assessment is completed, deployers must notify the designated market surveillance authority of the results, including any identified risks and the steps taken to address them. This notification supports regulatory oversight by enabling authorities to monitor and intervene if fundamental rights are at risk. Deployers must update the FRIA regularly and whenever significant changes to the AI system or its use occur, ensuring ongoing protection of fundamental rights throughout the system’s deployment.\n\n**Compliance Task List:**\n- [ ] Conduct a comprehensive fundamental rights impact assessment for each high-risk AI system deployed, focusing on risks related to privacy, non-discrimination, freedom of expression, and other fundamental rights.\n- [ ] Identify and document specific risks of harm associated with the AI system’s design, functionality, and context of use.\n- [ ] Develop and implement mitigation measures to address and minimize identified fundamental rights risks.\n- [ ] Prepare detailed documentation of the assessment process, findings, and corrective actions taken.\n- [ ] Notify the relevant market surveillance authority of the FRIA results and any risk mitigation steps.\n- [ ] Update the fundamental rights impact assessment periodically and whenever material changes to the AI system or its deployment occur.\n- [ ] Maintain clear records of the FRIA and notifications for audit and inspection purposes.\n- [ ] Integrate the FRIA process with other compliance activities such as risk management, human oversight, and transparency obligations.\n- [ ] Train personnel involved in deployment on fundamental rights risks and assessment procedures to ensure informed oversight.",
    "requiredTags": [
      "role:deployer"
    ],
    "requiredAllTags": [
      "risk:high"
    ]
  },
  {
    "id": "o30",
    "article": "[Article 41](https://artificialintelligenceact.eu/article/41/)",
    "description": "### Obligation for Article 41 – Common Specifications  \n\nArticle 41 of the EU AI Act empowers the European Commission to adopt implementing acts that establish common technical specifications (\"common specifications\") for the requirements applicable to high-risk AI systems or general-purpose AI models. These common specifications serve as a regulatory tool to provide detailed and practical technical solutions to meet the Act’s requirements when harmonized standards are not available, are insufficient, or do not fully address fundamental rights concerns.\n\nProviders of high-risk AI systems or general-purpose AI models that conform to these common specifications are presumed to comply with the relevant legal requirements covered by those specifications. This presumption facilitates market access and regulatory acceptance, providing a clear compliance pathway. If providers opt not to or cannot comply with the common specifications, they must justify and demonstrate that their technical solutions achieve at least an equivalent level of compliance with the regulatory requirements.\n\nThe Commission adopts common specifications only after consulting advisory forums, relevant expert bodies, and within the procedures defined by EU standardization regulations. When harmonized standards become available and are published as references in the Official Journal of the European Union, the Commission will repeal the corresponding common specifications to prioritize harmonized standards.\n\n**Compliance Task List:**\n- [ ] Determine if common specifications have been adopted by the Commission for the AI system category relevant to your product.\n- [ ] If common specifications exist, assess whether your high-risk AI system or general-purpose AI model complies fully with these specifications.\n- [ ] Conformity with common specifications creates a presumption of compliance with the EU AI Act requirements covered by those specifications.\n- [ ] If your system does not fully conform to common specifications, prepare a substantiated justification demonstrating that your technical solutions meet or exceed the required level of compliance.\n- [ ] Maintain clear documentation of compliance or justification efforts related to common specifications for audit and regulatory inspection purposes.\n- [ ] Monitor updates or repeal of common specifications when harmonized standards are published and adapt compliance strategies accordingly.\n- [ ] Engage with relevant standardization and regulatory bodies as needed to stay informed about common specification developments.\n- [ ] Incorporate requirements from common specifications into technical documentation, risk management, quality management, and conformity assessment processes.\n- [ ] Cooperate with competent authorities and the European AI Office, providing access to compliance evidence related to common specifications when requested.",
    "requiredTags": [
      "risk:high",
      "risk:gpai"
    ],
    "requiredAllTags": [
      "role:provider"
    ]
  },
  {
    "id": "o31",
    "article": "[Article 43](https://artificialintelligenceact.eu/article/43/)",
    "description": "### Obligation for Article 43 – Conformity Assessments\n\nUnder Article 43 of the EU AI Act, providers of high-risk AI systems must demonstrate compliance with the regulation's requirements through conformity assessment procedures before placing their systems on the EU market or putting them into service. The conformity assessment process verifies that high-risk AI systems meet all applicable legal, technical, and ethical standards set out in Title III, Chapter 2 of the Act, including risk management, data governance, technical documentation, record-keeping, transparency, human oversight, accuracy, robustness, and cybersecurity.\n\nProviders have options for conducting conformity assessments based on whether they have applied harmonized standards or common specifications:\n\n- If harmonized standards (Article 40) or common specifications (Article 41) are fully applied, providers may choose:\n  - An internal control procedure (Annex VI), carried out internally by the provider; or\n  - A procedure involving a notified body that assesses the quality management system and technical documentation (Annex VII).\n\n- If harmonized standards or common specifications are not fully applied or unavailable, providers must follow the full conformity assessment procedure involving a notified body as per Annex VII.\n\nNotified bodies designated under relevant EU legislation perform audits, issue EU technical documentation assessment certificates, and have the authority to suspend or withdraw certificates if compliance is not maintained. For AI systems used by law enforcement, immigration, asylum authorities, or EU institutions, market surveillance authorities act as notified bodies.\n\nThe conformity assessment ensures providers take full responsibility for the safety, reliability, and compliance of high-risk AI systems, supporting regulatory oversight and public trust. It must be redone when substantial modifications affecting compliance or intended purpose occur.\n\n**Compliance Task List:**\n- [ ] Determine if your high-risk AI system applies harmonized standards or common specifications.\n- [ ] Select the appropriate conformity assessment procedure: internal control (Annex VI), notified body involvement (Annex VII), or full notified body procedure if standards are not fully applied.\n- [ ] Prepare and maintain comprehensive technical documentation and quality management systems supporting conformity assessment.\n- [ ] Engage notified bodies as required to perform assessments, audits, and certification.\n- [ ] Obtain and retain EU technical documentation assessment certificates and affix CE marking indicating conformity.\n- [ ] Ensure ongoing compliance and notify notified bodies of any significant system modifications requiring reassessment.\n- [ ] Cooperate with notified bodies and regulatory authorities throughout the conformity assessment and certification lifespan.\n- [ ] Keep informed of updates to harmonized standards, common specifications, and latest regulatory guidance impacting conformity assessment procedures.",
    "requiredTags": [
      "role:provider"
    ],
    "requiredAllTags": [
      "risk:high"
    ]
  },
  {
    "id": "o32",
    "article": "[Article 44](https://artificialintelligenceact.eu/article/44/)",
    "description": "### Obligation for Article 44 – Certificates and Extension of Validity  \n\nProviders of high-risk AI systems who have obtained certificates of conformity from notified bodies pursuant to Annex VII of the EU AI Act must be aware that such certificates are issued for a limited validity period. Specifically, certificates for AI systems covered by Annex I are valid for up to five years, while those for systems covered by Annex III are valid for up to four years. Providers may request extensions to the validity of these certificates beyond their initial period.\n\nExtensions are granted following a reassessment process that ensures the AI system still complies with the applicable conformity assessment requirements. Each extension period cannot exceed five years for Annex I systems and four years for Annex III systems. Supplements to certificates remain valid only while the core certificate is valid.\n\nIf during reassessment the notified body finds that the AI system no longer meets the requirements, it may suspend, withdraw, or restrict the certificate unless the provider takes corrective actions within an appropriate deadline. Providers have the right to appeal decisions by notified bodies regarding certificates.\n\nThis obligation ensures that conformity certificates remain current and reflect ongoing compliance throughout the lifecycle of high-risk AI systems, thus maintaining safety, reliability, and trust in the EU market.\n\n**Compliance Task List:**\n- [ ] Monitor the validity period of certificates issued by notified bodies for your high-risk AI systems.\n- [ ] Plan ahead to request extensions to certificate validity before expiration to avoid compliance gaps.\n- [ ] Prepare for and facilitate reassessment by notified bodies as required for certificate extension.\n- [ ] Ensure that your AI system continues to meet all conformity requirements during the reassessment.\n- [ ] Implement any necessary corrective actions promptly if notified bodies identify compliance issues during reassessment.\n- [ ] Keep all certificate documentation, including extensions and supplements, up to date and accessible.\n- [ ] Understand the appeal procedures available if you disagree with decisions made by notified bodies regarding certification.\n- [ ] Coordinate with notified bodies to align certificate validity with product life cycles and market presence.\n- [ ] Maintain records of all communications and assessments related to certificate validity and extensions.",
    "requiredTags": [
      "role:provider"
    ],
    "requiredAllTags": [
      "risk:high"
    ]
  },
  {
    "id": "o33",
    "article": "[Article 47](https://artificialintelligenceact.eu/article/47/)",
    "description": "### Obligation for Article 47 – EU Declaration of Conformity  \n\nProviders of high-risk AI systems must draw up and maintain an EU Declaration of Conformity before placing their systems on the EU market or putting them into service. This declaration is a formal document in which the provider declares that the AI system complies with all applicable requirements of the EU AI Act and relevant Union harmonisation legislation. The declaration serves as a legal attestation that the AI system has undergone the necessary conformity assessment procedures and meets the regulatory standards for safety, transparency, fundamental rights protection, and quality.\n\nThe provider must keep the EU Declaration of Conformity readily available and at the disposal of national competent authorities for at least 10 years after the AI system is placed on the market or put into service. This facilitates audits, inspections, and enforcement actions, ensuring ongoing regulatory oversight and accountability.\n\n**Compliance Task List:**\n- [ ] Prepare an EU Declaration of Conformity that clearly states your high-risk AI system’s compliance with all relevant requirements under the EU AI Act.\n- [ ] Ensure the declaration includes identification of the provider, a description of the AI system, conformity assessment details, and references to applicable standards or common specifications.\n- [ ] Sign and date the declaration according to legal requirements, designating an authorized representative of the provider.\n- [ ] Keep the original declaration and all related documentation organized and securely stored for at least 10 years after market placement or service commencement.\n- [ ] Make the declaration available to national competent authorities upon request for inspection or verification purposes.\n- [ ] Update the declaration promptly when modifications to the AI system or applicable regulations require a reassessment of compliance.\n- [ ] Integrate declaration management within your overall quality management and regulatory compliance processes.\n- [ ] Train relevant personnel on the importance and use of the EU Declaration of Conformity to ensure consistent compliance practices.\n- [ ] Coordinate with authorized representatives, importers, distributors, and other stakeholders to ensure the declaration accompanies the AI system as required.",
    "requiredTags": [
      "role:provider"
    ],
    "requiredAllTags": [
      "risk:high"
    ]
  },
  {
    "id": "o34",
    "article": "[Article 48](https://artificialintelligenceact.eu/article/48/)",
    "description": "### Obligation for Article 48 – CE Marking\n\nProviders of high-risk AI systems must affix the CE marking to their products before placing them on the EU market or putting them into service, demonstrating conformity with the EU AI Act's requirements. Article 48 sets out specific criteria ensuring the CE marking is visible, legible, and indelibly affixed to the AI system or its data carrier when feasible.\n\nThe CE marking serves as a visible indication to regulators, users, and other stakeholders that the AI system complies with all applicable EU legislation, including safety, fundamental rights, and quality obligations. It enables market surveillance authorities to quickly identify compliant products, facilitating trust and accountability.\n\nProviders must follow rules relating to the position, size, and clarity of the CE marking, ensuring consistency with other applicable EU legislation where the AI system may also fall under additional regulatory domains. If the CE marking cannot be affixed directly (e.g., digital-only AI systems), it must be placed on packaging or accompanying documentation.\n\n**Compliance Task List:**\n- [ ] Affix the CE marking on the AI system or its data carrier in a visible, legible, and indelible manner before placing it on the EU market or putting it into service.\n- [ ] Ensure the CE marking meets size, format, and positioning requirements consistent with EU directives and regulations.\n- [ ] Place the CE marking on packaging or user documentation if direct marking on the AI system is not possible.\n- [ ] Coordinate CE marking placement with other applicable EU regulatory marks when the AI system falls under multiple legislative frameworks.\n- [ ] Maintain evidence of conformity (e.g., technical documentation, declaration of conformity, certificates) that justifies the use of the CE marking.\n- [ ] Update CE marking procedures and documentation as needed following changes to the AI system or applicable regulatory requirements.\n- [ ] Train personnel responsible for regulatory compliance and product labeling on CE marking obligations and best practices.\n- [ ] Cooperate with competent authorities in case of market surveillance and verification of CE marking compliance.",
    "requiredTags": [
      "role:provider"
    ],
    "requiredAllTags": [
      "risk:high"
    ]
  },
  {
    "id": "o35",
    "article": "[Article 49](https://artificialintelligenceact.eu/article/49/)",
    "description": "### Obligation for Article 49 – Registration of Providers, Authorized Representatives, and Deployers  \n\nArticle 49 of the EU AI Act mandates that providers of high-risk AI systems, their authorized representatives (when applicable), and deployers must register themselves and their respective AI systems in the EU database established under Article 71. This registration requirement promotes transparency, traceability, and effective market surveillance throughout the lifecycle of high-risk AI technologies in the European Union.\n\nThe registration must include key information such as identification details of the provider, authorized representative, or deployer; the AI system’s characteristics, intended purpose, and classification; and evidence of compliance with applicable conformity assessment requirements. Registering in this centralized database enables competent authorities and the European AI Office to monitor high-risk AI systems efficiently, facilitate regulatory interventions, and support enforcement actions if needed.\n\nMaintaining accurate and up-to-date registration records is essential for ensuring continued market access, regulatory compliance, and fostering trust among users and regulators.\n\n**Compliance Task List:**\n- [ ] Register the provider’s identification details in the EU AI database before placing the high-risk AI system on the EU market or putting it into service.\n- [ ] Appoint and register authorized representatives within the EU, as required, including their contact and mandate details.\n- [ ] Deployers must also register themselves and provide relevant information about the AI systems they place into operation, particularly when required by specific use cases or regulatory instructions.\n- [ ] Enter comprehensive AI system data during registration, including classification, intended purpose, versions, and conformity assessment status.\n- [ ] Keep the registration information current by updating details promptly following any significant changes to the AI system, provider status, authorized representative, or deployment arrangements.\n- [ ] Ensure registration data is complete, accurate, and readily accessible for review by competent authorities and the European AI Office.\n- [ ] Monitor and comply with any technical or procedural requirements related to registration as outlined by the Commission or the European AI Office.\n- [ ] Maintain internal records of registration confirmations and related communications as part of the compliance documentation.\n- [ ] Coordinate registration efforts with other compliance obligations such as quality management, conformity assessment, and post-market monitoring.\n- [ ] Train relevant personnel on registration duties and procedures to ensure timely and proper fulfillment of this obligation.",
    "requiredTags": [
      "role:provider",
      "role:deployer",
      "role:authorized-representative"
    ],
    "requiredAllTags": [
      "risk:high"
    ]
  },
  {
    "id": "o36",
    "article": "[Article 72](https://artificialintelligenceact.eu/article/72/)",
    "description": "### Obligation for Article 72 – Post-Market Monitoring by Providers and Post-Market Monitoring Plan for High-Risk AI Systems  \n\nProviders of high-risk AI systems are required to establish and maintain a post-market monitoring system that is proportionate to the nature of the AI technology and the risks posed by the system. This system must actively, systematically, and continuously collect, document, and analyze relevant data about the AI system’s performance throughout its entire lifecycle. The data can come from multiple sources, including information provided by deployers or other external sources, enabling the provider to continuously evaluate the AI system’s compliance with the requirements set out in Chapter III, Section 2 of the EU AI Act.\n\nThe system should also, where relevant, analyze interactions with other AI systems. However, this obligation excludes sensitive operational data of deployers who are law enforcement authorities. The post-market monitoring must be based on a documented post-market monitoring plan, which forms part of the technical documentation required by Annex IV. The European Commission is expected to adopt detailed implementing provisions, including a template and required elements for this plan, by February 2, 2026.\n\nProviders with AI systems already subject to equivalent post-market monitoring under other EU harmonization legislation may integrate the required elements from Article 72 into their existing monitoring systems, provided the level of protection remains equivalent. This provision aims to streamline compliance and reduce duplication of efforts.\n\n**Compliance Task List:**\n- [ ] Establish and document a proportionate post-market monitoring system tailored to the AI system’s technology and risk profile.\n- [ ] Actively and systematically collect, document, and analyze data on the AI system’s performance and operation throughout its lifecycle.\n- [ ] Include data received from deployers or collected from other reliable sources for ongoing evaluation of compliance.\n- [ ] Assess the interaction of the AI system with other AI systems, where relevant.\n- [ ] Exclude sensitive operational data from law enforcement deployers in the monitoring process.\n- [ ] Develop and maintain a detailed post-market monitoring plan as part of the AI system’s technical documentation.\n- [ ] Incorporate the forthcoming European Commission’s template and required elements into the post-market monitoring plan once published.\n- [ ] Where applicable, integrate Article 72 requirements into existing post-market monitoring systems under other EU legislation, ensuring equivalent protection.\n- [ ] Keep post-market monitoring documentation current and ready for review by competent authorities.\n- [ ] Use findings from post-market monitoring to improve the AI system’s design, mitigate risks, and ensure ongoing conformity.\n- [ ] Train personnel responsible for managing post-market monitoring activities.\n- [ ] Cooperate with national competent authorities and the European AI Office by providing access to post-market monitoring data and documentation upon request.",
    "requiredTags": [
      "role:provider"
    ],
    "requiredAllTags": [
      "risk:high"
    ]
  },
  {
    "id": "o37",
    "article": "[Article 73](https://artificialintelligenceact.eu/article/73/)",
    "description": "### Obligation for Article 73 – Reporting of Serious Incidents  \n  \nProviders of high-risk AI systems are required to promptly report any serious incidents involving their systems to the market surveillance authority of the Member State where the incident occurred. Serious incidents are events that lead to or could lead to death, serious injury, or serious damage to health, or could cause significant damage to property or fundamental rights.\n\nThe reporting obligations are time-sensitive, with specific deadlines based on the severity and immediacy of the incident:\n\n- Providers must report immediately if the incident results in death or serious injury.\n- For less severe incidents that nonetheless pose serious risks, reporting must occur within a reasonable timeframe, but no later than 15 calendar days after becoming aware of the incident.\n- If the provider is unable to provide all required information in the initial report, they must submit supplementary information without undue delay as it becomes available.\n\nIn addition to reporting to the market surveillance authority, providers are responsible for taking appropriate corrective actions to address the cause of the serious incident, including withdrawing or disabling the AI system if necessary to prevent harm.\n\nThis obligation is critical to ensure rapid regulatory response, protect public safety, fundamental rights, and maintain trust in high-risk AI systems deployed within the EU.\n\n**Compliance Task List:**\n- [ ] Establish protocols to promptly detect and assess serious incidents related to high-risk AI systems.\n- [ ] Report serious incidents to the relevant market surveillance authority without delay upon becoming aware of incidents involving death, serious injury, or serious damage.\n- [ ] Ensure reporting within 15 calendar days for serious incidents that pose significant risks but do not involve death or serious injury.\n- [ ] Provide complete and accurate information about the incident, including details on the AI system, the nature of the incident, its consequences, and measures taken or planned.\n- [ ] Submit supplementary reports promptly if additional relevant information becomes available after the initial notification.\n- [ ] Implement corrective actions such as system withdrawal, disabling, or recall to mitigate risks stemming from the incident.\n- [ ] Maintain detailed records of serious incidents, reports, and corrective actions for regulatory review and audits.\n- [ ] Train personnel on the identification, reporting, and management of serious incidents according to Article 73 requirements.\n- [ ] Cooperate fully with market surveillance authorities and the European AI Office during investigations related to serious incidents.\n",
    "requiredTags": [
      "role:provider"
    ],
    "requiredAllTags": [
      "risk:high"
    ]
  },
  {
    "id": "o38",
    "article": " [Article 86](https://artificialintelligenceact.eu/article/86/)",
    "description": "### Obligation for Article 86 – Right to Explanation of Individual Decision-Making  \n\nArticle 86 grants any affected person the right to receive a clear and meaningful explanation from the deployer when they are subject to a decision made solely on the basis of a high-risk AI system that produces legal effects or similarly significant impacts. This right ensures transparency and accountability in automated decision-making processes, strengthening trust and protecting fundamental rights.\n\nDeployers must have processes in place to provide accessible, understandable explanations about the logic, significance, and envisaged consequences of the AI-driven decision. The explanation should be provided promptly upon request and must empower individuals to understand how the decision was derived, enabling them to seek review, appeal, or further information if necessary. This obligation complements other transparency and human oversight duties, ensuring that individuals retain effective control and recourse regarding AI-based decisions affecting them.\n\n**Compliance Task List:**\n- [ ] Establish procedures for affected persons to request explanations of individual decisions made by high-risk AI systems.\n- [ ] Provide clear, meaningful, and accessible explanations covering:\n  - The logic involved in the AI decision-making process.\n  - The significance and impact of the decision on the individual.\n  - The envisaged consequences resulting from the decision.\n- [ ] Ensure explanations are communicated in language and formats understandable to the affected persons, considering their context and needs (e.g., plain language, multiple languages, accessible formats).\n- [ ] Deliver explanations promptly following requests, adhering to any specified time limits or reasonable deadlines.\n- [ ] Maintain records of explanation requests and responses to demonstrate compliance.\n- [ ] Train relevant personnel on the obligation to provide explanations and appropriate communication approaches.\n- [ ] Integrate explanation protocols with broader transparency, human oversight, and data protection frameworks.\n- [ ] Facilitate channels for affected persons to seek additional information, challenge the decision, or access human review mechanisms as applicable.\n- [ ] Review and update explanation mechanisms regularly to incorporate feedback, legal developments, and best practices.\n- [ ] Cooperate with competent authorities regarding oversight of explanation obligations and transparency requirements.",
    "requiredTags": [
      "role:deployer"
    ],
    "requiredAllTags": [
      "risk:high"
    ]
  },
  {
    "id": "o39",
    "article": "[Article 52](https://artificialintelligenceact.eu/article/52/)",
    "description": "### Obligation for Article 52 – Procedure for Providers of General-Purpose AI Models  \n\nProviders of general-purpose AI models that meet the conditions set out in Article 51, particularly those qualifying as general-purpose AI models with systemic risk, must notify the European Commission without delay and within two weeks after meeting or becoming aware that they meet these criteria. The notification must include all necessary information demonstrating that the model satisfies the specified conditions.\n\nThe European Commission may then assess if the notified general-purpose AI model poses systemic risks. Providers have the right to submit substantiated arguments to demonstrate that despite meeting the criteria, their model does not present systemic risks, seeking exemption from classification as a systemic-risk model. If the Commission rejects such arguments, the model will be designated as one with systemic risk.\n\nMoreover, providers can request reassessment of this designation by the Commission at least six months after the decision, presenting new, objective, and detailed reasons for the request. The Commission maintains a public list of general-purpose AI models with systemic risk, ensuring transparency while respecting intellectual property and trade secrets.\n\n**Compliance Task List:**\n- [ ] Notify the European Commission promptly and in any case within two weeks upon meeting the conditions of Article 51 defining a general-purpose AI model with systemic risk.\n- [ ] Provide all required information in the notification to demonstrate compliance with the criteria for systemic-risk designation.\n- [ ] Prepare and submit substantiated arguments if seeking to contest the systemic-risk classification of the model.\n- [ ] Monitor Commission decisions on the model’s systemic risk status and comply with the designation.\n- [ ] If designated with systemic risk, understand the implications including additional obligations under the EU AI Act.\n- [ ] Request reassessment of the systemic-risk designation at appropriate intervals (minimum six months after designation), providing new and detailed justifications.\n- [ ] Keep records of notifications, submissions, and communications with the Commission.\n- [ ] Stay informed about updates to criteria and procedures as set out by Commission delegated acts and Annex XIII.\n- [ ] Coordinate compliance with other relevant provisions in the EU AI Act applicable to general-purpose AI models with or without systemic risk.\n",
    "requiredTags": [
      "role:provider"
    ],
    "requiredAllTags": [
      "risk:gpai"
    ]
  },
  {
    "id": "o40",
    "article": "[Article 53](https://artificialintelligenceact.eu/article/53/)",
    "description": "### Obligation for Article 53 – Obligations for Providers of General-Purpose AI Models  \n\nProviders of general-purpose AI models must draw up, maintain, and keep up-to-date comprehensive technical documentation that details their model’s development, training, testing, evaluation, and integration capabilities. This documentation is essential for demonstrating compliance with the EU AI Act, supports regulatory oversight, and enables downstream providers who integrate these models into AI systems to understand their capabilities and limitations.\n\nThe technical documentation must include, at a minimum, the elements set out in Annex XI, covering aspects such as the model’s intended tasks, design specifications, training data and methodologies, evaluation results, computational resources used, and energy consumption. Providers also need to prepare and make available detailed information to users and integrators of the model (per Annex XII), enabling them to comply with their own regulatory obligations while respecting intellectual property and trade secret protections.\n\nAdditionally, providers must establish policies to ensure compliance with EU copyright and related rights laws, and publish a sufficiently detailed summary of the content used to train the general-purpose AI model. Providers are required to cooperate with the European Commission and national authorities, and may rely on codes of practice or harmonized standards to demonstrate compliance.\n\nThis obligation excludes providers releasing models under free and open-source licenses that fully disclose their parameters and architecture, except for general-purpose AI models with systemic risks.\n\n**Compliance Task List:**\n\n- [ ] Draw up and continuously update detailed technical documentation of the general-purpose AI model, covering:\n  - The model’s intended tasks and types of AI systems where it can be integrated.\n  - Design and development details, including architecture, parameters, training methodologies, and rationale.\n  - Information on training, testing, and validation data, sourcing, curation, and bias detection measures.\n  - Computational resources used, training duration, and estimated energy consumption.\n  - Evaluation results and adversarial testing strategies where applicable.\n  \n- [ ] Provide comprehensive information and documentation to downstream providers who integrate the model, including interactions with hardware/software, software versions, and integration requirements, while protecting intellectual property and trade secrets.\n\n- [ ] Implement and maintain policies to comply with EU copyright and related rights laws, including respecting third-party rights and reservations.\n\n- [ ] Publish a sufficiently detailed, publicly available summary describing the content used for training the AI model in accordance with templates provided by the AI Office.\n\n- [ ] Cooperate promptly and fully with the European Commission and national competent authorities during supervision and compliance checks.\n\n- [ ] Maintain and update documentation in line with evolving technological developments, regulatory amendments, and delegated acts from the European Commission.\n\n- [ ] Adhere to or demonstrate alternative compliance if not following codes of practice or harmonized standards.\n\n- [ ] Train relevant personnel on obligations under Article 53 and maintain records evidencing compliance activities.",
    "requiredTags": [
      "role:provider"
    ],
    "requiredAllTags": [
      "risk:gpai"
    ]
  },
  {
    "id": "o41",
    "article": "[Article 54](https://artificialintelligenceact.eu/article/54/)",
    "description": "### Obligation for Article 54 – Authorized Representatives of Providers of General-Purpose AI Models  \n\nProviders of general-purpose AI models who are established outside the European Union (third countries) must appoint an authorized representative established within the EU prior to placing their models on the EU market or making them available in the EU. This appointment must be made via a written mandate empowering the authorized representative to perform all tasks specified by the EU AI Act related to ensuring compliance.\n\nThe authorized representative acts as the local point of contact for the European AI Office and national competent authorities for regulatory supervision, enforcement, and market surveillance. Their responsibilities include verifying that the provider has prepared the required technical documentation (as specified in Annex XI), ensuring documentation is maintained and accessible for at least 10 years, supplying information and documentation upon reasoned request, and cooperating with competent authorities during investigations or compliance checks.\n\nThe authorized representative must also be empowered to be addressed by authorities in place of or in addition to the provider. If the representative considers the provider is not fulfilling obligations under the AI Act, they must terminate the mandate and promptly inform the relevant market surveillance authority, providing reasons for termination.\n\nProviders offering general-purpose AI models under a free and open-source license—where parameters, architecture, and usage information are fully disclosed—are exempt from this obligation unless their models present systemic risks.\n\n**Compliance Task List:**\n- [ ] Before placing the general-purpose AI model on the EU market, appoint an authorized representative established in the EU by written mandate.\n- [ ] Empower the authorized representative via mandate to:\n  - Verify that the provider has drawn up the technical documentation required under Annex XI.\n  - Keep the technical documentation and contact details accessible to the AI Office and competent authorities for at least 10 years.\n  - Provide requested information and documentation to the AI Office and national authorities to demonstrate compliance.\n  - Cooperate fully with authorities for supervision, investigations, and enforcement actions.\n- [ ] Ensure the authorized representative can be addressed by competent authorities in addition to or instead of the provider.\n- [ ] Provide a copy of the mandate to the AI Office upon request in an official EU language.\n- [ ] Require the authorized representative to immediately terminate the mandate and notify the relevant market surveillance authority if the provider is non-compliant, explaining the reasons.\n- [ ] Except for certain free and open-source general-purpose AI models without systemic risks, comply with this requirement to maintain market access and regulatory conformity in the EU.",
    "requiredTags": [
      "role:provider",
      "role:authorized-representative"
    ],
    "requiredAllTags": [
      "risk:gpai"
    ]
  },
  {
    "id": "o42",
    "article": "[Article 56](https://artificialintelligenceact.eu/article/56/)",
    "description": "### Obligation for Article 56 – Codes of Practice for Providers of General-Purpose AI Models  \n\nArticle 56 establishes a framework for the development and adherence to voluntary codes of practice to help providers of general-purpose AI (GPAI) models and downstream providers comply with their obligations under the EU AI Act, particularly for transparency, copyright, and safety/security. These codes serve as a practical and interim tool to demonstrate compliance before formal European standards are adopted.\n\nThe EU AI Office plays a central role in encouraging, facilitating, and assessing the adequacy of these codes. The codes are developed through a multi-stakeholder process, engaging providers, downstream users, independent experts, civil society, and authorities to ensure broad input and balanced guidance.\n\nThough codes of practice are not legally binding, providers who adhere to them benefit by demonstrating good faith compliance with key obligations, including technical documentation, transparency towards downstream users, copyright compliance, and systemic risk management for high-risk models. Providers who do not follow the codes must prove equivalent compliance by other means, which may be more burdensome.\n\nThe Code of Practice typically includes three key sections:\n\n1. **Transparency**: Requirements for up-to-date documentation, training data summaries, and sharing essential information with downstream users.\n2. **Copyright**: Policies ensuring compliance with EU copyright laws when training models.\n3. **Safety and Security**: Additional measures for GPAI models with systemic risk, including risk assessments, incident reporting, and cybersecurity.\n\nDownstream providers are expected to adhere to and benefit from the code by receiving timely information enabling their own compliance obligations.\n\n**Compliance Task List:**\n\n- [ ] Participate in drawing up and adopting codes of practice facilitated by the EU AI Office, contributing expertise and feedback as relevant.\n- [ ] Review and adhere to the voluntary code of practice applicable to providers of general-purpose AI models, aligning internal processes with its guidance.\n- [ ] Maintain comprehensive, up-to-date technical documentation and transparency measures as outlined in the code, including sharing model information with downstream providers.\n- [ ] Implement policies to comply with EU copyright and related rights law as detailed in the code.\n- [ ] For models with systemic risk, follow enhanced safety and security commitments specified in the code.\n- [ ] Use the code as a structured pathway to demonstrate compliance with Articles 53 and 55 of the EU AI Act until harmonized standards come into effect.\n- [ ] Cooperate with the EU AI Office and national authorities in assessing adherence to the code and provide required information upon request.\n- [ ] Encourage downstream providers to adhere to the code’s principles and provide them with necessary documentation and support within prescribed timeframes.\n- [ ] Monitor updates or revisions to the code and adjust compliance measures accordingly.\n- [ ] Train relevant personnel on commitments and procedures derived from the code of practice to foster a culture of responsible AI governance.",
    "requiredTags": [
      "role:provider"
    ],
    "requiredAllTags": [
      "risk:gpai"
    ]
  },
  {
    "id": "o43",
    "article": "[Article 89](https://artificialintelligenceact.eu/article/89/)",
    "description": "### Obligation for Article 89 – Monitoring Actions by Downstream Providers and Complaint Mechanisms  \n\nArticle 89 empowers downstream providers of AI systems to actively participate in the regulatory ecosystem by monitoring compliance with the EU AI Act and lodging complaints when they believe that an AI system infringes the regulatory requirements. This provision facilitates decentralized oversight by enabling those directly integrating or deploying AI technologies to raise concerns about non-compliance, safety risks, or fundamental rights violations.\n\nDownstream providers have a right to notify competent authorities or the European AI Office of suspected infringements, supporting timely investigation and enforcement actions. This mechanism strengthens the overall integrity and effectiveness of the AI regulatory framework by incorporating practical frontline insights into compliance supervision.\n\nTo enable efficient and responsible lodging of complaints, downstream providers must establish clear internal procedures for identifying potential infringements, collecting relevant evidence, and communicating with authorities. Providers should also ensure protections against retaliation and promote transparency and cooperation in resolving compliance issues. Authorities are expected to treat complaints seriously, maintain confidentiality where appropriate, and provide feedback on their handling.\n\n**Compliance Task List:**\n- [ ] Develop and implement internal procedures for downstream providers to monitor AI systems in use for potential infringements of the EU AI Act.\n- [ ] Establish clear, accessible channels for staff and stakeholders to report suspected non-compliance, safety concerns, or rights violations related to AI systems.\n- [ ] Collect and document relevant evidence or observations supporting complaints, ensuring accuracy and confidentiality.\n- [ ] Formally lodge complaints with national competent authorities or the European AI Office when regulations appear to be violated.\n- [ ] Cooperate fully with authorities during investigations arising from lodged complaints, providing additional information as requested.\n- [ ] Promote awareness and training across teams about the right and responsibility to report infringements under Article 89.\n- [ ] Ensure procedures protect complainants against retaliation or unfair treatment within the organization.\n- [ ] Maintain records of complaints lodged, communications with authorities, and outcomes of investigations or resolutions.\n- [ ] Integrate complaint and monitoring practices into broader compliance, risk management, and governance frameworks.\n- [ ] Engage constructively with providers and authorities to help remediate identified non-compliance or safety issues.",
    "requiredTags": [
      "role:provider"
    ],
    "requiredAllTags": [
      "risk:gpai"
    ]
  },
  {
    "id": "o44",
    "article": "[Article 51](https://artificialintelligenceact.eu/article/51/)",
    "description": "### Obligation for Article 51 – Classification of General-Purpose AI (GPAI) Models as GPAI with Systemic Risk\n\nArticle 51 establishes the criteria and conditions under which a general-purpose AI model (GPAI) must be classified as a GPAI with systemic risk within the European Union. A GPAI model is identified as having systemic risk when its potential to cause significant adverse impacts on fundamental rights, public safety, or other critical societal interests is deemed substantial due to the model’s widespread use, technical capabilities, or application contexts.\n\nSystemic risk classification triggers additional regulatory obligations and scrutiny under the EU AI Act, including enhanced transparency, compliance, and monitoring requirements. This classification aims to address the unique challenges posed by powerful and broadly applicable AI models that have pervasive influence and could amplify harms if not properly managed.\n\nThe European Commission defines the specific conditions and thresholds for systemic risk designation through delegated acts, considering factors such as the scale of deployment, likelihood and severity of risks, potential for widespread harm, and the AI model’s role in critical infrastructure, decision-making, or social functions.\n\n**Compliance Task List:**\n- [ ] Monitor the criteria and delegated acts issued by the European Commission defining systemic risk conditions for GPAI models.\n- [ ] Evaluate whether your general-purpose AI model meets the conditions to be classified as having systemic risk, based on its capabilities, uses, and impact potential.\n- [ ] If classified as a systemic-risk GPAI model, comply with the additional obligations prescribed by the EU AI Act for such models, including notification to the Commission (Article 52), enhanced documentation (Article 53), and adherence to stricter compliance and oversight regimes.\n- [ ] Maintain detailed records and evidence of the classification assessment process and communications with regulatory authorities.\n- [ ] Prepare to submit required notifications and documentation promptly upon meeting systemic risk criteria.\n- [ ] Implement risk mitigation, transparency, human oversight, and post-market monitoring measures consistent with systemic-risk status.\n- [ ] Stay informed on updates to classification criteria and regulatory guidance issued by the European Commission and the European AI Office.\n- [ ] Train relevant personnel on the implications of systemic risk classification and associated compliance responsibilities.\n- [ ] Facilitate cooperation with the European Commission, the European AI Office, and national competent authorities regarding systemic risk oversight.\n",
    "requiredAllTags": [
      "risk:GPAI-Systemic",
      "role:provider"
    ]
  },
  {
    "id": "o45",
    "article": "[Article 55](https://artificialintelligenceact.eu/article/55/)",
    "description": "### Obligation for Article 55 – Obligations for Providers of General-Purpose AI Models with Systemic Risk  \n\nProviders of general-purpose AI (GPAI) models classified as having systemic risk must comply with enhanced obligations beyond those applicable to all GPAI models. These obligations aim to identify, assess, mitigate, and monitor systemic risks at the Union level that may arise from the development, deployment, or use of systemic-risk GPAI models. The focus is on safeguarding public health, safety, fundamental rights, and societal well-being from adverse impacts that are significant and widespread.\n\nKey duties include performing rigorous and state-of-the-art model evaluations through standardized protocols and tools, including adversarial testing, specifically designed to detect and reduce systemic risks. Providers must continuously assess systemic risks originating from the model itself or its wider use context, and implement appropriate mitigation strategies.\n\nProviders are also required to keep detailed records, document serious incidents related to their systemic-risk GPAI models, and report these incidents without undue delay to the European AI Office and relevant national competent authorities. Additionally, they must ensure an adequate level of cybersecurity for the AI model and its associated physical infrastructure to prevent unauthorized access, tampering, or exploitation.\n\nUntil harmonized standards are published, providers may demonstrate compliance by adhering to approved codes of practice. Those not following such codes or standards must provide alternative evidence of adequate compliance for evaluation by the European Commission.\n\n**Compliance Task List:**\n- [ ] Perform detailed model evaluations using standardized, state-of-the-art protocols and tools, including adversarial testing, to identify and mitigate systemic risks in the GPAI model.\n- [ ] Continuously assess and address possible systemic risks at the EU level, including risks emerging from the development, marketing, or use of the systemic-risk GPAI model.\n- [ ] Track, document, and report serious incidents and relevant corrective measures promptly to the European AI Office and appropriate national authorities.\n- [ ] Implement and maintain robust cybersecurity measures protecting the systemic-risk GPAI model and its physical infrastructure from threats such as unauthorized access and attacks.\n- [ ] Leverage voluntary codes of practice recognized under Article 56 to demonstrate compliance until harmonized standards become available.\n- [ ] If not adhering to approved codes of practice or harmonized standards, develop and submit alternative adequate compliance evidence for Commission assessment.\n- [ ] Manage all information and documentation related to these obligations, including sensitive trade secrets, in accordance with confidentiality rules set out in Article 78 of the EU AI Act.\n- [ ] Train personnel involved in the development, deployment, and monitoring of systemic-risk GPAI models to ensure awareness and competence regarding these obligations.\n- [ ] Stay updated on evolving regulatory requirements, standards, codes of practice, and best practices associated with systemic risk management for GPAI models.",
    "requiredTags": [
      "role:provider"
    ],
    "requiredAllTags": [
      "risk:GPAI-Systemic"
    ]
  }
]